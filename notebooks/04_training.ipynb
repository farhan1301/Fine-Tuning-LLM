{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Model Fine-Tuning with QLoRA\n",
    "\n",
    "This notebook fine-tunes Llama 3.2 1B using QLoRA (Quantized Low-Rank Adaptation) on CPU.\n",
    "\n",
    "## What we'll do:\n",
    "1. Load the base model with 8-bit quantization\n",
    "2. Configure LoRA for parameter-efficient fine-tuning\n",
    "3. Set up training configuration (optimized for CPU)\n",
    "4. Train the model\n",
    "5. Save the fine-tuned model\n",
    "6. Monitor training progress\n",
    "\n",
    "## Expected Training Time:\n",
    "- **~12-24 hours** on Intel MacBook Pro 2018 (CPU-only)\n",
    "- You can pause and resume training if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    TrainerCallback,\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    PeftModel,\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Key parameters optimized for CPU training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "MODEL_ID = \"meta-llama/Llama-3.2-1B\"\n",
    "OUTPUT_DIR = \"../models/llama-3.2-1b-brd-extraction\"\n",
    "FINAL_MODEL_DIR = \"../models/final/llama-3.2-1b-brd-final\"\n",
    "\n",
    "# LoRA configuration (optimized for CPU)\n",
    "LORA_CONFIG = {\n",
    "    \"r\": 8,  # Rank - lower for faster training\n",
    "    \"lora_alpha\": 16,  # Scaling factor\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"target_modules\": [\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Training configuration (CPU-optimized)\n",
    "TRAINING_CONFIG = {\n",
    "    \"num_train_epochs\": 3,\n",
    "    \"per_device_train_batch_size\": 1,  # Must be 1 on CPU\n",
    "    \"gradient_accumulation_steps\": 32,  # Effective batch size = 32\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"max_seq_length\": 2048,\n",
    "    \"warmup_steps\": 100,\n",
    "    \"logging_steps\": 10,\n",
    "    \"save_steps\": 100,\n",
    "    \"save_total_limit\": 3,\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Model: {MODEL_ID}\")\n",
    "print(f\"  LoRA Rank: {LORA_CONFIG['r']}\")\n",
    "print(f\"  Epochs: {TRAINING_CONFIG['num_train_epochs']}\")\n",
    "print(f\"  Effective Batch Size: {TRAINING_CONFIG['gradient_accumulation_steps']}\")\n",
    "print(f\"  Max Sequence Length: {TRAINING_CONFIG['max_seq_length']}\")\n",
    "print(\"\\n✓ Configuration set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load prepared datasets\n",
    "data_files = {\n",
    "    \"train\": \"../data/processed/train.json\",\n",
    "    \"validation\": \"../data/processed/validation.json\",\n",
    "}\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=data_files)\n",
    "\n",
    "print(\"Dataset loaded:\")\n",
    "print(f\"  Training samples: {len(dataset['train'])}\")\n",
    "print(f\"  Validation samples: {len(dataset['validation'])}\")\n",
    "print(\"\\n✓ Dataset ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Model and Tokenizer with 8-bit Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading model with 8-bit quantization...\")\n",
    "print(\"This may take a few minutes...\\n\")\n",
    "\n",
    "# Configure 8-bit quantization\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_threshold=6.0,\n",
    ")\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(\"✓ Model loaded\")\n",
    "print(f\"  Memory footprint: {model.get_memory_footprint() / 1e9:.2f} GB\")\n",
    "print(f\"  Total parameters: {model.num_parameters() / 1e6:.0f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepare Model for Training with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Preparing model for training...\\n\")\n",
    "\n",
    "# Prepare for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_CONFIG[\"r\"],\n",
    "    lora_alpha=LORA_CONFIG[\"lora_alpha\"],\n",
    "    target_modules=LORA_CONFIG[\"target_modules\"],\n",
    "    lora_dropout=LORA_CONFIG[\"lora_dropout\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "print(\"Trainable Parameters:\")\n",
    "print(\"=\" * 60)\n",
    "model.print_trainable_parameters()\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n✓ Model prepared with LoRA\")\n",
    "print(\"  Only ~0.5-2% of parameters will be trained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Configure Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Training arguments (optimized for CPU)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=TRAINING_CONFIG[\"num_train_epochs\"],\n",
    "    per_device_train_batch_size=TRAINING_CONFIG[\"per_device_train_batch_size\"],\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=TRAINING_CONFIG[\"gradient_accumulation_steps\"],\n",
    "    learning_rate=TRAINING_CONFIG[\"learning_rate\"],\n",
    "    warmup_steps=TRAINING_CONFIG[\"warmup_steps\"],\n",
    "    logging_steps=TRAINING_CONFIG[\"logging_steps\"],\n",
    "    save_steps=TRAINING_CONFIG[\"save_steps\"],\n",
    "    save_total_limit=TRAINING_CONFIG[\"save_total_limit\"],\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    fp16=False,  # CPU doesn't support fp16\n",
    "    bf16=False,  # Use fp32 on CPU\n",
    "    gradient_checkpointing=True,  # Save memory\n",
    "    optim=\"adamw_torch\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_grad_norm=0.3,\n",
    "    report_to=\"none\",  # Change to \"tensorboard\" if you want logging\n",
    "    logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    ")\n",
    "\n",
    "print(\"Training Arguments:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Output Directory: {OUTPUT_DIR}\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Batch Size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Gradient Accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Effective Batch Size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Learning Rate: {training_args.learning_rate}\")\n",
    "print(f\"  Warmup Steps: {training_args.warmup_steps}\")\n",
    "print(f\"  Gradient Checkpointing: {training_args.gradient_checkpointing}\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n✓ Training arguments configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Custom Callback for Progress Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProgressCallback(TrainerCallback):\n",
    "    \"\"\"Custom callback to track and display training progress.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.training_start = None\n",
    "        self.best_eval_loss = float('inf')\n",
    "    \n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        self.training_start = datetime.now()\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"TRAINING STARTED\")\n",
    "        print(f\"Start Time: {self.training_start.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs:\n",
    "            # Calculate progress\n",
    "            total_steps = state.max_steps\n",
    "            current_step = state.global_step\n",
    "            progress = (current_step / total_steps) * 100\n",
    "            \n",
    "            # Time elapsed\n",
    "            elapsed = datetime.now() - self.training_start\n",
    "            \n",
    "            # Display key metrics\n",
    "            if 'loss' in logs:\n",
    "                print(f\"Step {current_step}/{total_steps} ({progress:.1f}%) | \"\n",
    "                      f\"Loss: {logs['loss']:.4f} | \"\n",
    "                      f\"Elapsed: {str(elapsed).split('.')[0]}\")\n",
    "            \n",
    "            if 'eval_loss' in logs:\n",
    "                eval_loss = logs['eval_loss']\n",
    "                if eval_loss < self.best_eval_loss:\n",
    "                    self.best_eval_loss = eval_loss\n",
    "                    print(f\"  ★ New best eval loss: {eval_loss:.4f}\")\n",
    "    \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        elapsed = datetime.now() - self.training_start\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"TRAINING COMPLETED\")\n",
    "        print(f\"Total Time: {str(elapsed).split('.')[0]}\")\n",
    "        print(f\"Best Eval Loss: {self.best_eval_loss:.4f}\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"✓ Progress callback created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initializing trainer...\\n\")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    max_seq_length=TRAINING_CONFIG[\"max_seq_length\"],\n",
    "    dataset_text_field=\"text\",\n",
    "    packing=False,  # Safer for structured output tasks\n",
    "    callbacks=[ProgressCallback()],\n",
    ")\n",
    "\n",
    "print(\"✓ Trainer initialized\")\n",
    "print(f\"\\nEstimated training steps: {trainer.args.max_steps}\")\n",
    "print(f\"Estimated time on CPU: 12-24 hours\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Start Training\n",
    "\n",
    "**Important Notes:**\n",
    "- Training will take 12-24 hours on CPU\n",
    "- You can interrupt and resume from last checkpoint\n",
    "- Monitor the loss - it should decrease over time\n",
    "- Checkpoints are saved every 100 steps\n",
    "\n",
    "**To resume from checkpoint:**\n",
    "```python\n",
    "trainer.train(resume_from_checkpoint=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training...\")\n",
    "print(\"This will take approximately 12-24 hours on CPU.\")\n",
    "print(\"You can stop training with Ctrl+C and resume later.\\n\")\n",
    "\n",
    "# Start training\n",
    "train_result = trainer.train()\n",
    "\n",
    "# Save training metrics\n",
    "metrics = train_result.metrics\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "\n",
    "print(\"\\n✓ Training completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluate on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating on validation set...\\n\")\n",
    "\n",
    "eval_metrics = trainer.evaluate()\n",
    "\n",
    "print(\"Validation Metrics:\")\n",
    "print(\"=\" * 60)\n",
    "for key, value in eval_metrics.items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "trainer.log_metrics(\"eval\", eval_metrics)\n",
    "trainer.save_metrics(\"eval\", eval_metrics)\n",
    "\n",
    "print(\"\\n✓ Evaluation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Saving final model...\\n\")\n",
    "\n",
    "# Create final model directory\n",
    "Path(FINAL_MODEL_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save model and tokenizer\n",
    "trainer.save_model(FINAL_MODEL_DIR)\n",
    "tokenizer.save_pretrained(FINAL_MODEL_DIR)\n",
    "\n",
    "# Save training configuration\n",
    "training_info = {\n",
    "    \"base_model\": MODEL_ID,\n",
    "    \"training_date\": datetime.now().isoformat(),\n",
    "    \"lora_config\": LORA_CONFIG,\n",
    "    \"training_config\": TRAINING_CONFIG,\n",
    "    \"final_metrics\": eval_metrics,\n",
    "    \"training_samples\": len(dataset[\"train\"]),\n",
    "    \"validation_samples\": len(dataset[\"validation\"]),\n",
    "}\n",
    "\n",
    "with open(f\"{FINAL_MODEL_DIR}/training_info.json\", \"w\") as f:\n",
    "    json.dump(training_info, f, indent=2)\n",
    "\n",
    "print(f\"✓ Model saved to: {FINAL_MODEL_DIR}\")\n",
    "print(f\"✓ Training info saved\")\n",
    "print(\"\\nFiles saved:\")\n",
    "print(f\"  - Model weights (LoRA adapters)\")\n",
    "print(f\"  - Tokenizer\")\n",
    "print(f\"  - Training configuration\")\n",
    "print(f\"  - Metrics and logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Quick Test of Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing fine-tuned model...\\n\")\n",
    "\n",
    "# Sample BRD for testing\n",
    "test_brd = \"\"\"Business Requirements Document\n",
    "Project: Customer Portal Development\n",
    "\n",
    "We need to build a web-based customer portal for our e-commerce platform. The portal will allow customers to view order history, track shipments, and manage their accounts. The project requires 2 full-stack developers working for 10 weeks. Total estimated effort is 400 hours with a budget of $50,000.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"### Instruction:\n",
    "Extract the project estimation fields from the following Business Requirements Document.\n",
    "Return a JSON object with these exact fields: effort_hours (number), timeline_weeks (number), cost_usd (number).\n",
    "Return ONLY the JSON object, no additional text.\n",
    "\n",
    "### Input:\n",
    "{test_brd}\n",
    "\n",
    "### Output:\n",
    "\"\"\"\n",
    "\n",
    "# Generate\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.1,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "output = generated_text.split(\"### Output:\")[-1].strip()\n",
    "\n",
    "print(\"Test BRD:\")\n",
    "print(\"=\"*80)\n",
    "print(test_brd)\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nExtracted JSON:\")\n",
    "print(\"=\"*80)\n",
    "print(output)\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n✓ Fine-tuned model is working!\")\n",
    "print(\"\\nNote: For production use, integrate with Pydantic AI (see notebook 06)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Visualize Training Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "# Load training logs\n",
    "try:\n",
    "    with open(f\"{OUTPUT_DIR}/trainer_state.json\", \"r\") as f:\n",
    "        state = json.load(f)\n",
    "    \n",
    "    # Extract loss history\n",
    "    train_loss = [(log['step'], log['loss']) for log in state['log_history'] if 'loss' in log]\n",
    "    eval_loss = [(log['step'], log['eval_loss']) for log in state['log_history'] if 'eval_loss' in log]\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Training loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    if train_loss:\n",
    "        steps, losses = zip(*train_loss)\n",
    "        plt.plot(steps, losses, 'b-', linewidth=2)\n",
    "        plt.title('Training Loss', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Steps')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Evaluation loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    if eval_loss:\n",
    "        steps, losses = zip(*eval_loss)\n",
    "        plt.plot(steps, losses, 'r-', linewidth=2, marker='o')\n",
    "        plt.title('Validation Loss', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Steps')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{FINAL_MODEL_DIR}/training_curves.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"✓ Training curves saved\")\nexcept FileNotFoundError:\n",
    "    print(\"Training state not found. This is normal if training was just started.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What we've done:\n",
    "- ✓ Loaded Llama 3.2 1B with 8-bit quantization\n",
    "- ✓ Configured QLoRA for parameter-efficient fine-tuning\n",
    "- ✓ Set up CPU-optimized training configuration\n",
    "- ✓ Trained the model on BRD extraction task\n",
    "- ✓ Evaluated on validation set\n",
    "- ✓ Saved fine-tuned model and configuration\n",
    "- ✓ Tested the fine-tuned model\n",
    "- ✓ Visualized training progress\n",
    "\n",
    "### Training Techniques Used:\n",
    "- **QLoRA**: Quantized Low-Rank Adaptation for efficient training\n",
    "- **8-bit Quantization**: Reduced memory footprint for CPU training\n",
    "- **Gradient Accumulation**: Simulated larger batch sizes\n",
    "- **Gradient Checkpointing**: Reduced memory usage\n",
    "- **Cosine Learning Rate Schedule**: Smooth convergence\n",
    "\n",
    "### Model Performance:\n",
    "- Only **~0.5-2%** of parameters were trained (LoRA adapters)\n",
    "- Final validation loss: *[see metrics above]*\n",
    "- Model can now extract structured JSON from BRDs\n",
    "\n",
    "### Next Steps:\n",
    "Move on to `05_evaluation.ipynb` for comprehensive model evaluation.\n",
    "\n",
    "### Notes:\n",
    "- Training on CPU is slow but effective for 1B models\n",
    "- LoRA adapters are small (~10-50 MB) and portable\n",
    "- You can share just the adapters, not the full model\n",
    "- For inference, load base model + LoRA adapters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
