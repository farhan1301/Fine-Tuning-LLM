{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Environment Setup and Model Verification\n",
    "\n",
    "This notebook sets up the environment for fine-tuning Llama 3.2 1B on CPU.\n",
    "\n",
    "## What we'll do:\n",
    "1. Install required packages\n",
    "2. Verify hardware and PyTorch configuration\n",
    "3. Download Llama 3.2 1B model\n",
    "4. Test 8-bit quantization\n",
    "5. Run a simple generation test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Required Packages\n",
    "\n",
    "**Note:** Run this once, then restart the kernel before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages from requirements.txt\n",
    "!pip install -q -r ../requirements.txt\n",
    "\n",
    "print(\"✓ Installation complete! Please restart the kernel before continuing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Verify Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import peft\n",
    "import trl\n",
    "import platform\n",
    "import sys\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ENVIRONMENT INFORMATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Python and System Info\n",
    "print(f\"Python Version: {sys.version.split()[0]}\")\n",
    "print(f\"Platform: {platform.system()} {platform.release()}\")\n",
    "print(f\"Processor: {platform.processor()}\")\n",
    "print()\n",
    "\n",
    "# Package Versions\n",
    "print(\"PACKAGE VERSIONS:\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Transformers: {transformers.__version__}\")\n",
    "print(f\"PEFT: {peft.__version__}\")\n",
    "print(f\"TRL: {trl.__version__}\")\n",
    "print()\n",
    "\n",
    "# Device Information\n",
    "print(\"DEVICE INFORMATION:\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"Running on CPU (expected for Intel MacBook)\")\n",
    "\n",
    "# Check MPS (Apple Silicon) - won't be available on Intel Mac\n",
    "if hasattr(torch.backends, 'mps'):\n",
    "    print(f\"MPS Available: {torch.backends.mps.is_available()}\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download and Load Llama 3.2 1B\n",
    "\n",
    "**Important:** You'll need to:\n",
    "1. Have a Hugging Face account\n",
    "2. Accept the Llama 3.2 license at: https://huggingface.co/meta-llama/Llama-3.2-1B\n",
    "3. Create an access token at: https://huggingface.co/settings/tokens\n",
    "4. Login using the token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Login to Hugging Face\n",
    "# You'll be prompted to enter your token\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Model with 8-bit Quantization\n",
    "\n",
    "We'll use 8-bit quantization which is more stable on CPU than 4-bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "print(\"Loading model with 8-bit quantization...\")\n",
    "print(\"This may take a few minutes on first run (downloading model)...\\n\")\n",
    "\n",
    "# Configure 8-bit quantization\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_threshold=6.0,\n",
    ")\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(\"✓ Model loaded successfully!\")\n",
    "print(f\"\\nModel size: {model.get_memory_footprint() / 1e9:.2f} GB\")\n",
    "print(f\"Total parameters: {model.num_parameters() / 1e6:.0f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Model Generation\n",
    "\n",
    "Let's verify the model works by generating some text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompt = \"\"\"Extract the project estimation from this BRD:\n",
    "\n",
    "Business Requirements Document\n",
    "Project: Mobile App Development\n",
    "\n",
    "The project requires 3 developers for 8 weeks.\n",
    "Estimated effort: 480 hours\n",
    "Budget: $48,000\n",
    "\n",
    "Answer in JSON format:\"\"\"\n",
    "\n",
    "print(\"Testing model generation...\\n\")\n",
    "print(\"Prompt:\")\n",
    "print(\"-\" * 60)\n",
    "print(test_prompt)\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Generate\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.7,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\nGenerated Output:\")\n",
    "print(\"-\" * 60)\n",
    "print(generated_text)\n",
    "print(\"-\" * 60)\n",
    "print(\"\\n✓ Model generation working!\")\n",
    "print(\"Note: The base model may not produce perfect JSON yet.\")\n",
    "print(\"After fine-tuning, it will extract structured data reliably.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Verify LoRA Configuration\n",
    "\n",
    "Test that we can prepare the model for PEFT training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "print(\"Testing LoRA configuration...\\n\")\n",
    "\n",
    "# Prepare model for training\n",
    "model_for_training = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # Rank\n",
    "    lora_alpha=16,  # Scaling\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "peft_model = get_peft_model(model_for_training, lora_config)\n",
    "\n",
    "print(\"LoRA Configuration:\")\n",
    "print(\"-\" * 60)\n",
    "peft_model.print_trainable_parameters()\n",
    "print(\"-\" * 60)\n",
    "print(\"\\n✓ LoRA setup successful!\")\n",
    "print(\"Only ~0.5-2% of parameters will be trained.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Setup Summary\n",
    "\n",
    "Let's create a summary of our setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "setup_info = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"model_id\": model_id,\n",
    "    \"quantization\": \"8-bit\",\n",
    "    \"device\": \"cpu\",\n",
    "    \"pytorch_version\": torch.__version__,\n",
    "    \"transformers_version\": transformers.__version__,\n",
    "    \"peft_version\": peft.__version__,\n",
    "    \"model_size_gb\": round(model.get_memory_footprint() / 1e9, 2),\n",
    "    \"total_parameters\": model.num_parameters(),\n",
    "    \"lora_config\": {\n",
    "        \"rank\": 8,\n",
    "        \"alpha\": 16,\n",
    "        \"dropout\": 0.05,\n",
    "        \"target_modules\": lora_config.target_modules,\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save setup info\n",
    "with open(\"../configs/setup_info.json\", \"w\") as f:\n",
    "    json.dump(setup_info, f, indent=2)\n",
    "\n",
    "print(\"Setup Information:\")\n",
    "print(\"=\" * 60)\n",
    "print(json.dumps(setup_info, indent=2))\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n✓ Setup information saved to configs/setup_info.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Complete!\n",
    "\n",
    "### What we've verified:\n",
    "- ✓ All required packages installed\n",
    "- ✓ Llama 3.2 1B downloaded and loaded\n",
    "- ✓ 8-bit quantization working on CPU\n",
    "- ✓ Model can generate text\n",
    "- ✓ LoRA configuration tested\n",
    "\n",
    "### Next Steps:\n",
    "Move on to `02_data_generation.ipynb` to create synthetic BRD training data.\n",
    "\n",
    "### Notes:\n",
    "- The base model may not produce perfect JSON yet - that's expected\n",
    "- After fine-tuning, it will extract structured data reliably\n",
    "- Model is loaded in 8-bit to reduce memory usage on CPU\n",
    "- Training will be slow on CPU but feasible for 1B model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
