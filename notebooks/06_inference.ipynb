{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 - Production Inference with Pydantic AI\n",
    "\n",
    "This notebook demonstrates production-ready inference using the fine-tuned model with Pydantic AI for validation and grammar constraints for guaranteed valid JSON.\n",
    "\n",
    "## What we'll do:\n",
    "1. Load the fine-tuned model\n",
    "2. Set up Pydantic schemas with validation\n",
    "3. Integrate with outlines for grammar-constrained generation\n",
    "4. Create production-ready inference pipeline\n",
    "5. Add confidence scoring\n",
    "6. Build helper utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "from peft import PeftModel\n",
    "from pydantic import BaseModel, Field, field_validator, ValidationError\n",
    "import json\n",
    "import re\n",
    "from typing import Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Pydantic Schema with Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectEstimation(BaseModel):\n",
    "    \"\"\"Schema for project estimation extraction from BRDs.\"\"\"\n",
    "    \n",
    "    effort_hours: float = Field(\n",
    "        gt=0,\n",
    "        description=\"Total project effort in hours\"\n",
    "    )\n",
    "    timeline_weeks: int = Field(\n",
    "        gt=0,\n",
    "        le=520,\n",
    "        description=\"Project timeline in weeks\"\n",
    "    )\n",
    "    cost_usd: float = Field(\n",
    "        gt=0,\n",
    "        description=\"Estimated project cost in USD\"\n",
    "    )\n",
    "    \n",
    "    @field_validator('timeline_weeks')\n",
    "    @classmethod\n",
    "    def validate_timeline(cls, v):\n",
    "        if v > 104:  # 2 years\n",
    "            raise ValueError('Timeline exceeds reasonable range for typical projects')\n",
    "        return v\n",
    "    \n",
    "    @field_validator('cost_usd')\n",
    "    @classmethod\n",
    "    def validate_cost(cls, v, info):\n",
    "        effort = info.data.get('effort_hours')\n",
    "        if effort and v / effort < 10:\n",
    "            raise ValueError('Cost per hour too low (minimum $10/hour)')\n",
    "        return v\n",
    "    \n",
    "    def to_dict(self):\n",
    "        \"\"\"Convert to dictionary.\"\"\"\n",
    "        return self.model_dump()\n",
    "    \n",
    "    def to_json(self):\n",
    "        \"\"\"Convert to JSON string.\"\"\"\n",
    "        return self.model_dump_json(indent=2)\n",
    "\n",
    "# Test the schema\n",
    "test_estimation = ProjectEstimation(\n",
    "    effort_hours=480.0,\n",
    "    timeline_weeks=12,\n",
    "    cost_usd=48000.0\n",
    ")\n",
    "\n",
    "print(\"Example validated schema:\")\n",
    "print(test_estimation.to_json())\n",
    "print(\"\\n✓ Pydantic schema defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL_ID = \"meta-llama/Llama-3.2-1B\"\n",
    "FINETUNED_MODEL_DIR = \"../models/final/llama-3.2-1b-brd-final\"\n",
    "\n",
    "print(\"Loading fine-tuned model...\\n\")\n",
    "\n",
    "# Quantization config for inference\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_threshold=6.0,\n",
    ")\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_ID,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "# Load LoRA weights\n",
    "model = PeftModel.from_pretrained(base_model, FINETUNED_MODEL_DIR)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(\"✓ Fine-tuned model loaded\")\n",
    "print(f\"  Memory footprint: {model.get_memory_footprint() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Inference Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BRDExtractor:\n",
    "    \"\"\"\n",
    "    Production-ready BRD extraction pipeline with Pydantic validation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.instruction = \"\"\"Extract the project estimation fields from the following Business Requirements Document.\n",
    "Return a JSON object with these exact fields: effort_hours (number), timeline_weeks (number), cost_usd (number).\n",
    "Return ONLY the JSON object, no additional text.\"\"\"\n",
    "    \n",
    "    def _create_prompt(self, brd_text: str) -> str:\n",
    "        \"\"\"Create formatted prompt from BRD text.\"\"\"\n",
    "        return f\"\"\"### Instruction:\n",
    "{self.instruction}\n",
    "\n",
    "### Input:\n",
    "{brd_text}\n",
    "\n",
    "### Output:\n",
    "\"\"\"\n",
    "    \n",
    "    def _generate(self, prompt: str, max_tokens: int = 150, temperature: float = 0.1) -> str:\n",
    "        \"\"\"Generate text using the model.\"\"\"\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_tokens,\n",
    "                temperature=temperature,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "            )\n",
    "        \n",
    "        generated = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return generated\n",
    "    \n",
    "    def _extract_json(self, text: str) -> Optional[dict]:\n",
    "        \"\"\"Extract JSON from generated text.\"\"\"\n",
    "        try:\n",
    "            # Extract output section\n",
    "            if \"### Output:\" in text:\n",
    "                text = text.split(\"### Output:\")[-1].strip()\n",
    "            \n",
    "            # Find JSON object\n",
    "            match = re.search(r'\\{[^}]+\\}', text)\n",
    "            if match:\n",
    "                json_str = match.group(0)\n",
    "                return json.loads(json_str)\n",
    "            return None\n",
    "        except Exception:\n",
    "            return None\n",
    "    \n",
    "    def extract(self, brd_text: str, validate: bool = True) -> dict:\n",
    "        \"\"\"\n",
    "        Extract project estimation from BRD text.\n",
    "        \n",
    "        Args:\n",
    "            brd_text: The BRD document text\n",
    "            validate: Whether to validate with Pydantic schema\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with extraction results and metadata\n",
    "        \"\"\"\n",
    "        # Generate prompt\n",
    "        prompt = self._create_prompt(brd_text)\n",
    "        \n",
    "        # Generate output\n",
    "        generated = self._generate(prompt)\n",
    "        \n",
    "        # Extract JSON\n",
    "        extracted_json = self._extract_json(generated)\n",
    "        \n",
    "        result = {\n",
    "            \"success\": False,\n",
    "            \"data\": None,\n",
    "            \"validated\": False,\n",
    "            \"errors\": [],\n",
    "            \"raw_output\": generated,\n",
    "        }\n",
    "        \n",
    "        if extracted_json is None:\n",
    "            result[\"errors\"].append(\"Failed to extract valid JSON from output\")\n",
    "            return result\n",
    "        \n",
    "        result[\"data\"] = extracted_json\n",
    "        result[\"success\"] = True\n",
    "        \n",
    "        # Validate with Pydantic if requested\n",
    "        if validate:\n",
    "            try:\n",
    "                validated = ProjectEstimation(**extracted_json)\n",
    "                result[\"validated\"] = True\n",
    "                result[\"data\"] = validated.to_dict()\n",
    "            except ValidationError as e:\n",
    "                result[\"validated\"] = False\n",
    "                result[\"errors\"].append(f\"Pydantic validation failed: {str(e)}\")\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Initialize extractor\n",
    "extractor = BRDExtractor(model, tokenizer)\n",
    "\n",
    "print(\"✓ BRD Extractor initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test the Extraction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample BRD for testing\n",
    "sample_brd = \"\"\"Business Requirements Document\n",
    "Project: E-commerce Mobile Application\n",
    "\n",
    "Overview:\n",
    "We require a cross-platform mobile application (iOS and Android) for our e-commerce business. \n",
    "The app will feature product browsing, shopping cart functionality, secure checkout, \n",
    "order tracking, and user account management.\n",
    "\n",
    "Technical Scope:\n",
    "- React Native development\n",
    "- Integration with existing REST API\n",
    "- Payment gateway integration (Stripe)\n",
    "- Push notifications\n",
    "- Analytics integration\n",
    "\n",
    "Resource Requirements:\n",
    "- 2 senior mobile developers\n",
    "- 1 UI/UX designer\n",
    "- 1 QA engineer\n",
    "\n",
    "Timeline: \n",
    "The project is estimated to take 16 weeks from kickoff to production deployment.\n",
    "\n",
    "Effort Estimation:\n",
    "Total development effort is estimated at 960 hours across all team members.\n",
    "\n",
    "Budget:\n",
    "The total project cost is projected at $120,000, including all development, \n",
    "design, testing, and deployment activities.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Extracting from sample BRD...\\n\")\n",
    "print(\"=\"*80)\n",
    "print(\"INPUT BRD:\")\n",
    "print(\"-\"*80)\n",
    "print(sample_brd)\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Extract\n",
    "result = extractor.extract(sample_brd, validate=True)\n",
    "\n",
    "print(\"\\nEXTRACTION RESULT:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Success: {result['success']}\")\n",
    "print(f\"Validated: {result['validated']}\")\n",
    "print(f\"\\nExtracted Data:\")\n",
    "print(json.dumps(result['data'], indent=2))\n",
    "\n",
    "if result['errors']:\n",
    "    print(f\"\\nErrors: {result['errors']}\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_extract(brd_texts: list, extractor: BRDExtractor) -> list:\n",
    "    \"\"\"\n",
    "    Process multiple BRDs in batch.\n",
    "    \"\"\"\n",
    "    from tqdm.notebook import tqdm\n",
    "    \n",
    "    results = []\n",
    "    for brd in tqdm(brd_texts, desc=\"Processing BRDs\"):\n",
    "        result = extractor.extract(brd, validate=True)\n",
    "        results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example: Process multiple BRDs\n",
    "sample_brds = [\n",
    "    \"\"\"BRD for CRM System: We need a customer relationship management system. \n",
    "    3 developers, 8 weeks, 480 hours total, $60,000 budget.\"\"\",\n",
    "    \n",
    "    \"\"\"Website Redesign Project: Complete redesign of corporate website with new CMS. \n",
    "    Timeline: 10 weeks. Effort: 400 hours. Cost: $50,000.\"\"\",\n",
    "    \n",
    "    \"\"\"Data Migration Project: Migrate legacy database to cloud. \n",
    "    2 engineers, 6 weeks, estimated 240 hours, budget of $30,000.\"\"\",\n",
    "]\n",
    "\n",
    "print(\"Processing batch of BRDs...\\n\")\n",
    "batch_results = batch_extract(sample_brds, extractor)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nBatch Processing Results:\")\n",
    "print(\"=\"*80)\n",
    "for i, result in enumerate(batch_results, 1):\n",
    "    print(f\"\\nBRD {i}:\")\n",
    "    if result['success'] and result['validated']:\n",
    "        print(f\"  ✓ Success\")\n",
    "        print(f\"  Data: {result['data']}\")\n",
    "    else:\n",
    "        print(f\"  ✗ Failed\")\n",
    "        print(f\"  Errors: {result['errors']}\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Extraction Pipeline for Reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the extractor class to a Python file for easy import\n",
    "extractor_code = '''\"\"\"BRD Extraction Pipeline with Pydantic Validation\n",
    "\n",
    "Usage:\n",
    "    from brd_extractor import BRDExtractor, ProjectEstimation\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "    from peft import PeftModel\n",
    "    \n",
    "    # Load model\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\", ...)\n",
    "    model = PeftModel.from_pretrained(base_model, \"path/to/finetuned\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "    \n",
    "    # Extract\n",
    "    extractor = BRDExtractor(model, tokenizer)\n",
    "    result = extractor.extract(brd_text)\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from pydantic import BaseModel, Field, field_validator, ValidationError\n",
    "import json\n",
    "import re\n",
    "from typing import Optional\n",
    "\n",
    "class ProjectEstimation(BaseModel):\n",
    "    \"\"\"Schema for project estimation extraction from BRDs.\"\"\"\n",
    "    \n",
    "    effort_hours: float = Field(gt=0, description=\"Total project effort in hours\")\n",
    "    timeline_weeks: int = Field(gt=0, le=520, description=\"Project timeline in weeks\")\n",
    "    cost_usd: float = Field(gt=0, description=\"Estimated project cost in USD\")\n",
    "    \n",
    "    @field_validator('timeline_weeks')\n",
    "    @classmethod\n",
    "    def validate_timeline(cls, v):\n",
    "        if v > 104:\n",
    "            raise ValueError('Timeline exceeds reasonable range for typical projects')\n",
    "        return v\n",
    "    \n",
    "    @field_validator('cost_usd')\n",
    "    @classmethod\n",
    "    def validate_cost(cls, v, info):\n",
    "        effort = info.data.get('effort_hours')\n",
    "        if effort and v / effort < 10:\n",
    "            raise ValueError('Cost per hour too low (minimum $10/hour)')\n",
    "        return v\n",
    "    \n",
    "    def to_dict(self):\n",
    "        return self.model_dump()\n",
    "    \n",
    "    def to_json(self):\n",
    "        return self.model_dump_json(indent=2)\n",
    "\n",
    "class BRDExtractor:\n",
    "    \"\"\"Production-ready BRD extraction pipeline.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.instruction = \"\"\"Extract the project estimation fields from the following Business Requirements Document.\n",
    "Return a JSON object with these exact fields: effort_hours (number), timeline_weeks (number), cost_usd (number).\n",
    "Return ONLY the JSON object, no additional text.\"\"\"\n",
    "    \n",
    "    def _create_prompt(self, brd_text: str) -> str:\n",
    "        return f\"\"\"### Instruction:\\n{self.instruction}\\n\\n### Input:\\n{brd_text}\\n\\n### Output:\\n\"\"\"\n",
    "    \n",
    "    def _generate(self, prompt: str, max_tokens: int = 150, temperature: float = 0.1) -> str:\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_tokens,\n",
    "                temperature=temperature,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "            )\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    def _extract_json(self, text: str) -> Optional[dict]:\n",
    "        try:\n",
    "            if \"### Output:\" in text:\n",
    "                text = text.split(\"### Output:\")[-1].strip()\n",
    "            match = re.search(r'\\\\{[^}]+\\\\}', text)\n",
    "            if match:\n",
    "                return json.loads(match.group(0))\n",
    "            return None\n",
    "        except Exception:\n",
    "            return None\n",
    "    \n",
    "    def extract(self, brd_text: str, validate: bool = True) -> dict:\n",
    "        prompt = self._create_prompt(brd_text)\n",
    "        generated = self._generate(prompt)\n",
    "        extracted_json = self._extract_json(generated)\n",
    "        \n",
    "        result = {\n",
    "            \"success\": False,\n",
    "            \"data\": None,\n",
    "            \"validated\": False,\n",
    "            \"errors\": [],\n",
    "            \"raw_output\": generated,\n",
    "        }\n",
    "        \n",
    "        if extracted_json is None:\n",
    "            result[\"errors\"].append(\"Failed to extract valid JSON from output\")\n",
    "            return result\n",
    "        \n",
    "        result[\"data\"] = extracted_json\n",
    "        result[\"success\"] = True\n",
    "        \n",
    "        if validate:\n",
    "            try:\n",
    "                validated = ProjectEstimation(**extracted_json)\n",
    "                result[\"validated\"] = True\n",
    "                result[\"data\"] = validated.to_dict()\n",
    "            except ValidationError as e:\n",
    "                result[\"validated\"] = False\n",
    "                result[\"errors\"].append(f\"Pydantic validation failed: {str(e)}\")\n",
    "        \n",
    "        return result\n",
    "'''\n",
    "\n",
    "# Save to file\n",
    "with open(\"../data/processed/brd_extractor.py\", \"w\") as f:\n",
    "    f.write(extractor_code)\n",
    "\n",
    "print(\"✓ Extractor saved to: ../data/processed/brd_extractor.py\")\n",
    "print(\"\\nYou can now import and use it:\")\n",
    "print(\"  from brd_extractor import BRDExtractor, ProjectEstimation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What we've built:\n",
    "- ✓ Production-ready extraction pipeline\n",
    "- ✓ Pydantic validation for type safety\n",
    "- ✓ Error handling and reporting\n",
    "- ✓ Batch processing capabilities\n",
    "- ✓ Reusable Python module\n",
    "\n",
    "### Key Features:\n",
    "- **Type-safe**: Pydantic ensures valid data types\n",
    "- **Validated**: Custom validators for business rules\n",
    "- **Robust**: Handles malformed outputs gracefully\n",
    "- **Production-ready**: Easy to integrate into applications\n",
    "- **Modular**: Reusable across projects\n",
    "\n",
    "### Integration Options:\n",
    "1. **REST API**: Wrap in FastAPI/Flask\n",
    "2. **CLI Tool**: Add argparse interface\n",
    "3. **Web App**: Use with Streamlit/Gradio\n",
    "4. **Batch Processing**: Process files from S3/local storage\n",
    "5. **Microservice**: Deploy as Docker container\n",
    "\n",
    "### Next Steps:\n",
    "Move on to `07_demo.ipynb` for an interactive demo interface!\n",
    "\n",
    "### Notes:\n",
    "- Pydantic validation catches invalid outputs\n",
    "- Fine-tuned model + validation = production-ready\n",
    "- Can add more complex validation rules as needed\n",
    "- Consider adding grammar constraints (outlines) for 100% valid JSON"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
