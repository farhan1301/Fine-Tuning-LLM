{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Data Preparation for Training\n",
    "\n",
    "This notebook prepares the synthetic BRD data for fine-tuning.\n",
    "\n",
    "## What we'll do:\n",
    "1. Load the generated BRD dataset\n",
    "2. Format data for instruction tuning\n",
    "3. Create train/validation/test splits\n",
    "4. Validate data quality\n",
    "5. Create Pydantic schemas\n",
    "6. Save formatted datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Generated Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import random\n",
    "from typing import Dict, List\n",
    "from pydantic import BaseModel, Field, field_validator\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set random seed\n",
    "random.seed(42)\n",
    "\n",
    "# Find the most recent complete dataset\n",
    "data_dir = Path(\"../data/synthetic_brds\")\n",
    "dataset_files = list(data_dir.glob(\"complete_dataset_*.json\"))\n",
    "\n",
    "if not dataset_files:\n",
    "    raise FileNotFoundError(\"No complete dataset found. Run 02_data_generation.ipynb first.\")\n",
    "\n",
    "# Load most recent\n",
    "latest_file = max(dataset_files, key=lambda p: p.stat().st_mtime)\n",
    "print(f\"Loading dataset: {latest_file.name}\")\n",
    "\n",
    "with open(latest_file, \"r\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "print(f\"✓ Loaded {len(dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Pydantic Schema\n",
    "\n",
    "This schema will be used for validation and later with Pydantic AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectEstimation(BaseModel):\n",
    "    \"\"\"Schema for project estimation extraction from BRDs.\"\"\"\n",
    "    \n",
    "    effort_hours: float = Field(\n",
    "        gt=0,\n",
    "        description=\"Total project effort in hours\"\n",
    "    )\n",
    "    timeline_weeks: int = Field(\n",
    "        gt=0,\n",
    "        le=520,  # Max 10 years\n",
    "        description=\"Project timeline in weeks\"\n",
    "    )\n",
    "    cost_usd: float = Field(\n",
    "        gt=0,\n",
    "        description=\"Estimated project cost in USD\"\n",
    "    )\n",
    "    \n",
    "    @field_validator('timeline_weeks')\n",
    "    @classmethod\n",
    "    def validate_timeline(cls, v):\n",
    "        if v > 104:  # 2 years\n",
    "            raise ValueError('Timeline exceeds reasonable range for typical projects')\n",
    "        return v\n",
    "    \n",
    "    @field_validator('cost_usd')\n",
    "    @classmethod\n",
    "    def validate_cost(cls, v, info):\n",
    "        effort = info.data.get('effort_hours')\n",
    "        if effort and v / effort < 10:\n",
    "            raise ValueError('Cost per hour too low (minimum $10/hour)')\n",
    "        return v\n",
    "\n",
    "# Test the schema\n",
    "test_estimation = ProjectEstimation(\n",
    "    effort_hours=480.0,\n",
    "    timeline_weeks=12,\n",
    "    cost_usd=48000.0\n",
    ")\n",
    "\n",
    "print(\"Pydantic Schema:\")\n",
    "print(test_estimation.model_dump_json(indent=2))\n",
    "print(\"\\n✓ Schema defined and validated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Validate All Labels\n",
    "\n",
    "Ensure all ground truth labels pass our Pydantic validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Validating all labels...\\n\")\n",
    "\n",
    "valid_samples = []\n",
    "invalid_samples = []\n",
    "\n",
    "for sample in dataset:\n",
    "    try:\n",
    "        # Validate with Pydantic\n",
    "        ProjectEstimation(**sample[\"labels\"])\n",
    "        valid_samples.append(sample)\n",
    "    except Exception as e:\n",
    "        invalid_samples.append({\n",
    "            \"sample_id\": sample[\"id\"],\n",
    "            \"error\": str(e),\n",
    "            \"labels\": sample[\"labels\"]\n",
    "        })\n",
    "\n",
    "print(f\"Valid samples: {len(valid_samples)}\")\n",
    "print(f\"Invalid samples: {len(invalid_samples)}\")\n",
    "\n",
    "if invalid_samples:\n",
    "    print(\"\\nFirst few invalid samples:\")\n",
    "    for inv in invalid_samples[:3]:\n",
    "        print(f\"  ID {inv['sample_id']}: {inv['error']}\")\n",
    "\n",
    "# Use only valid samples\n",
    "dataset = valid_samples\n",
    "print(f\"\\n✓ Using {len(dataset)} validated samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Format Data for Instruction Tuning\n",
    "\n",
    "Convert to the instruction-following format expected by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_for_training(sample: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Format a sample for instruction tuning.\n",
    "    \n",
    "    Uses the Alpaca/Llama instruction format:\n",
    "    ### Instruction:\n",
    "    ### Input:\n",
    "    ### Output:\n",
    "    \"\"\"\n",
    "    instruction = \"\"\"Extract the project estimation fields from the following Business Requirements Document.\n",
    "Return a JSON object with these exact fields: effort_hours (number), timeline_weeks (number), cost_usd (number).\n",
    "Return ONLY the JSON object, no additional text.\"\"\"\n",
    "    \n",
    "    input_text = sample[\"brd_text\"]\n",
    "    \n",
    "    output_json = json.dumps(sample[\"labels\"], ensure_ascii=False)\n",
    "    \n",
    "    # Combine in instruction format\n",
    "    formatted_text = f\"\"\"### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input_text}\n",
    "\n",
    "### Output:\n",
    "{output_json}\"\"\"\n",
    "    \n",
    "    return {\n",
    "        \"text\": formatted_text,\n",
    "        \"id\": sample[\"id\"],\n",
    "        \"metadata\": sample.get(\"metadata\", {})\n",
    "    }\n",
    "\n",
    "# Format all samples\n",
    "formatted_dataset = [format_for_training(sample) for sample in dataset]\n",
    "\n",
    "print(\"Example formatted sample:\")\n",
    "print(\"=\" * 80)\n",
    "print(formatted_dataset[0][\"text\"])\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n✓ Formatted {len(formatted_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Train/Val/Test Splits\n",
    "\n",
    "Split: 80% train, 10% validation, 10% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First split: 80% train, 20% temp\n",
    "train_data, temp_data = train_test_split(\n",
    "    formatted_dataset,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Second split: 50% val, 50% test (from temp)\n",
    "val_data, test_data = train_test_split(\n",
    "    temp_data,\n",
    "    test_size=0.5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Dataset Splits:\")\n",
    "print(f\"  Training:   {len(train_data):4d} samples ({len(train_data)/len(formatted_dataset)*100:.1f}%)\")\n",
    "print(f\"  Validation: {len(val_data):4d} samples ({len(val_data)/len(formatted_dataset)*100:.1f}%)\")\n",
    "print(f\"  Test:       {len(test_data):4d} samples ({len(test_data)/len(formatted_dataset)*100:.1f}%)\")\n",
    "print(f\"  Total:      {len(formatted_dataset):4d} samples\")\n",
    "\n",
    "print(\"\\n✓ Data split created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyze Data Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Extract labels from original dataset for analysis\n",
    "df = pd.DataFrame([sample[\"labels\"] for sample in dataset])\n",
    "\n",
    "print(\"Label Statistics:\")\n",
    "print(\"=\" * 80)\n",
    "print(df.describe())\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Text length analysis\n",
    "text_lengths = [len(sample[\"text\"]) for sample in formatted_dataset]\n",
    "print(f\"\\nText Length Statistics:\")\n",
    "print(f\"  Mean:   {sum(text_lengths) / len(text_lengths):.0f} characters\")\n",
    "print(f\"  Min:    {min(text_lengths)} characters\")\n",
    "print(f\"  Max:    {max(text_lengths)} characters\")\n",
    "print(f\"  Median: {sorted(text_lengths)[len(text_lengths)//2]} characters\")\n",
    "\n",
    "# Visualize distributions\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].hist(df[\"effort_hours\"], bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_title(\"Effort Hours Distribution\")\n",
    "axes[0].set_xlabel(\"Effort (hours)\")\n",
    "axes[0].set_ylabel(\"Frequency\")\n",
    "\n",
    "axes[1].hist(df[\"timeline_weeks\"], bins=30, edgecolor='black', alpha=0.7, color='green')\n",
    "axes[1].set_title(\"Timeline Distribution\")\n",
    "axes[1].set_xlabel(\"Timeline (weeks)\")\n",
    "axes[1].set_ylabel(\"Frequency\")\n",
    "\n",
    "axes[2].hist(df[\"cost_usd\"], bins=30, edgecolor='black', alpha=0.7, color='orange')\n",
    "axes[2].set_title(\"Cost Distribution\")\n",
    "axes[2].set_xlabel(\"Cost (USD)\")\n",
    "axes[2].set_ylabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../data/processed/label_distributions.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Statistics visualized and saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Formatted Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Create output directory\n",
    "output_dir = Path(\"../data/processed\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save splits as JSON\n",
    "splits = {\n",
    "    \"train\": train_data,\n",
    "    \"validation\": val_data,\n",
    "    \"test\": test_data\n",
    "}\n",
    "\n",
    "for split_name, split_data in splits.items():\n",
    "    output_file = output_dir / f\"{split_name}.json\"\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(split_data, f, indent=2)\n",
    "    print(f\"✓ Saved {split_name}: {len(split_data)} samples -> {output_file}\")\n",
    "\n",
    "# Also save in JSONL format (one JSON per line, efficient for training)\n",
    "for split_name, split_data in splits.items():\n",
    "    output_file = output_dir / f\"{split_name}.jsonl\"\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for sample in split_data:\n",
    "            f.write(json.dumps(sample) + \"\\n\")\n",
    "    print(f\"✓ Saved {split_name}: {len(split_data)} samples -> {output_file} (JSONL)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Pydantic Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save schema definition for later use\n",
    "schema_file = output_dir / \"pydantic_schema.py\"\n",
    "\n",
    "schema_code = '''\"\"\"Pydantic schema for BRD project estimation extraction.\"\"\"\n",
    "\n",
    "from pydantic import BaseModel, Field, field_validator\n",
    "\n",
    "class ProjectEstimation(BaseModel):\n",
    "    \"\"\"Schema for project estimation extraction from BRDs.\"\"\"\n",
    "    \n",
    "    effort_hours: float = Field(\n",
    "        gt=0,\n",
    "        description=\"Total project effort in hours\"\n",
    "    )\n",
    "    timeline_weeks: int = Field(\n",
    "        gt=0,\n",
    "        le=520,\n",
    "        description=\"Project timeline in weeks\"\n",
    "    )\n",
    "    cost_usd: float = Field(\n",
    "        gt=0,\n",
    "        description=\"Estimated project cost in USD\"\n",
    "    )\n",
    "    \n",
    "    @field_validator(\"timeline_weeks\")\n",
    "    @classmethod\n",
    "    def validate_timeline(cls, v):\n",
    "        if v > 104:  # 2 years\n",
    "            raise ValueError(\"Timeline exceeds reasonable range for typical projects\")\n",
    "        return v\n",
    "    \n",
    "    @field_validator(\"cost_usd\")\n",
    "    @classmethod\n",
    "    def validate_cost(cls, v, info):\n",
    "        effort = info.data.get(\"effort_hours\")\n",
    "        if effort and v / effort < 10:\n",
    "            raise ValueError(\"Cost per hour too low (minimum $10/hour)\")\n",
    "        return v\n",
    "\n",
    "# JSON Schema (for use with outlines/instructor)\n",
    "ESTIMATION_JSON_SCHEMA = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"effort_hours\": {\n",
    "            \"type\": \"number\",\n",
    "            \"minimum\": 0,\n",
    "            \"exclusiveMinimum\": True\n",
    "        },\n",
    "        \"timeline_weeks\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"minimum\": 1,\n",
    "            \"maximum\": 520\n",
    "        },\n",
    "        \"cost_usd\": {\n",
    "            \"type\": \"number\",\n",
    "            \"minimum\": 0,\n",
    "            \"exclusiveMinimum\": True\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"effort_hours\", \"timeline_weeks\", \"cost_usd\"],\n",
    "    \"additionalProperties\": False\n",
    "}\n",
    "'''\n",
    "\n",
    "with open(schema_file, \"w\") as f:\n",
    "    f.write(schema_code)\n",
    "\n",
    "print(f\"✓ Pydantic schema saved to: {schema_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Create Dataset Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {\n",
    "    \"created_at\": datetime.now().isoformat(),\n",
    "    \"total_samples\": len(formatted_dataset),\n",
    "    \"splits\": {\n",
    "        \"train\": len(train_data),\n",
    "        \"validation\": len(val_data),\n",
    "        \"test\": len(test_data)\n",
    "    },\n",
    "    \"label_statistics\": {\n",
    "        \"effort_hours\": {\n",
    "            \"mean\": float(df[\"effort_hours\"].mean()),\n",
    "            \"std\": float(df[\"effort_hours\"].std()),\n",
    "            \"min\": float(df[\"effort_hours\"].min()),\n",
    "            \"max\": float(df[\"effort_hours\"].max())\n",
    "        },\n",
    "        \"timeline_weeks\": {\n",
    "            \"mean\": float(df[\"timeline_weeks\"].mean()),\n",
    "            \"std\": float(df[\"timeline_weeks\"].std()),\n",
    "            \"min\": int(df[\"timeline_weeks\"].min()),\n",
    "            \"max\": int(df[\"timeline_weeks\"].max())\n",
    "        },\n",
    "        \"cost_usd\": {\n",
    "            \"mean\": float(df[\"cost_usd\"].mean()),\n",
    "            \"std\": float(df[\"cost_usd\"].std()),\n",
    "            \"min\": float(df[\"cost_usd\"].min()),\n",
    "            \"max\": float(df[\"cost_usd\"].max())\n",
    "        }\n",
    "    },\n",
    "    \"text_statistics\": {\n",
    "        \"mean_length\": sum(text_lengths) / len(text_lengths),\n",
    "        \"min_length\": min(text_lengths),\n",
    "        \"max_length\": max(text_lengths)\n",
    "    },\n",
    "    \"format\": \"instruction_tuning\",\n",
    "    \"instruction_template\": \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Output:\\n{output}\"\n",
    "}\n",
    "\n",
    "metadata_file = output_dir / \"dataset_metadata.json\"\n",
    "with open(metadata_file, \"w\") as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(\"Dataset Metadata:\")\n",
    "print(\"=\" * 80)\n",
    "print(json.dumps(metadata, indent=2))\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n✓ Metadata saved to: {metadata_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Verify Data is Ready for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data Preparation Checklist:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "checks = [\n",
    "    (\"✓\", f\"Total samples: {len(formatted_dataset)}\"),\n",
    "    (\"✓\", f\"Training samples: {len(train_data)}\"),\n",
    "    (\"✓\", f\"Validation samples: {len(val_data)}\"),\n",
    "    (\"✓\", f\"Test samples: {len(test_data)}\"),\n",
    "    (\"✓\", \"All labels validated with Pydantic\"),\n",
    "    (\"✓\", \"Instruction tuning format applied\"),\n",
    "    (\"✓\", \"Train/val/test splits created (80/10/10)\"),\n",
    "    (\"✓\", \"Data saved in JSON and JSONL formats\"),\n",
    "    (\"✓\", \"Pydantic schema saved\"),\n",
    "    (\"✓\", \"Dataset metadata created\"),\n",
    "    (\"✓\", \"Statistics and visualizations generated\"),\n",
    "]\n",
    "\n",
    "for symbol, check in checks:\n",
    "    print(f\"{symbol} {check}\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n✅ Data preparation complete! Ready for training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What we've done:\n",
    "- ✓ Loaded synthetic BRD dataset\n",
    "- ✓ Defined Pydantic schema for validation\n",
    "- ✓ Validated all ground truth labels\n",
    "- ✓ Formatted data for instruction tuning\n",
    "- ✓ Created train/val/test splits (80/10/10)\n",
    "- ✓ Generated dataset statistics and visualizations\n",
    "- ✓ Saved data in multiple formats (JSON, JSONL)\n",
    "- ✓ Created metadata and documentation\n",
    "\n",
    "### Files Created:\n",
    "- `data/processed/train.json` and `train.jsonl`\n",
    "- `data/processed/validation.json` and `validation.jsonl`\n",
    "- `data/processed/test.json` and `test.jsonl`\n",
    "- `data/processed/pydantic_schema.py`\n",
    "- `data/processed/dataset_metadata.json`\n",
    "- `data/processed/label_distributions.png`\n",
    "\n",
    "### Next Steps:\n",
    "Move on to `04_training.ipynb` to fine-tune the model with QLoRA!\n",
    "\n",
    "### Notes:\n",
    "- Data is in instruction-tuning format for optimal learning\n",
    "- All labels pass Pydantic validation\n",
    "- Dataset is balanced and diverse\n",
    "- Ready for efficient training with TRL's SFTTrainer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
