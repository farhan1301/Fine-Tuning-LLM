{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Colab Setup for LLM Fine-tuning\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/farhan1301/Fine-Tuning-LLM/blob/main/colab/00_colab_setup.ipynb)\n",
    "\n",
    "This notebook sets up Google Colab for fine-tuning Llama 3.2 1B.\n",
    "\n",
    "**GPU:** Make sure you have GPU enabled!\n",
    "- Go to: Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí T4 GPU\n",
    "\n",
    "**Estimated Time:** 5-10 minutes for setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"GPU Available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"GPU Memory:\", torch.cuda.get_device_properties(0).total_memory / 1e9, \"GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected! Go to Runtime ‚Üí Change runtime type ‚Üí T4 GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mount Google Drive (to save models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create project directory\n",
    "import os\n",
    "PROJECT_DIR = '/content/drive/MyDrive/LLM_Finetuning'\n",
    "os.makedirs(PROJECT_DIR, exist_ok=True)\n",
    "os.makedirs(f\"{PROJECT_DIR}/data\", exist_ok=True)\n",
    "os.makedirs(f\"{PROJECT_DIR}/models\", exist_ok=True)\n",
    "\n",
    "print(f\"‚úì Project directory: {PROJECT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%capture\n# Install NumPy first to avoid binary incompatibility\n!pip install -q 'numpy<2.0'\n# Install packages (suppress output)\n!pip install -q -U transformers>=4.40.0\n!pip install -q -U peft>=0.10.0\n!pip install -q -U trl>=0.8.0\n!pip install -q -U bitsandbytes>=0.41.0\n!pip install -q -U accelerate>=0.28.0\n!pip install -q -U datasets>=2.14.0\n!pip install -q -U pydantic>=2.5.0\n!pip install -q -U anthropic\n!pip install -q -U gradio\n\nprint(\"‚úì All packages installed\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Verify Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import peft\n",
    "import trl\n",
    "import torch\n",
    "\n",
    "print(\"Package Versions:\")\n",
    "print(f\"  PyTorch: {torch.__version__}\")\n",
    "print(f\"  Transformers: {transformers.__version__}\")\n",
    "print(f\"  PEFT: {peft.__version__}\")\n",
    "print(f\"  TRL: {trl.__version__}\")\n",
    "print(f\"\\n‚úì All packages loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Set Up API Keys\n",
    "\n",
    "**Important:** Add your API keys to Colab Secrets:\n",
    "1. Click the key icon (üîë) in left sidebar\n",
    "2. Add secrets:\n",
    "   - `HUGGINGFACE_TOKEN` - from https://huggingface.co/settings/tokens\n",
    "   - `ANTHROPIC_API_KEY` - from https://console.anthropic.com/\n",
    "\n",
    "Or use the form below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "try:\n",
    "    # Try to get from Colab secrets\n",
    "    HUGGINGFACE_TOKEN = userdata.get('HUGGINGFACE_TOKEN')\n",
    "    ANTHROPIC_API_KEY = userdata.get('ANTHROPIC_API_KEY')\n",
    "    print(\"‚úì API keys loaded from Colab secrets\")\n",
    "except:\n",
    "    # Manual input if secrets not set\n",
    "    print(\"‚ö†Ô∏è Colab secrets not found. Enter manually:\")\n",
    "    from getpass import getpass\n",
    "    HUGGINGFACE_TOKEN = getpass(\"Enter Hugging Face token: \")\n",
    "    ANTHROPIC_API_KEY = getpass(\"Enter Anthropic API key: \")\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['HUGGINGFACE_TOKEN'] = HUGGINGFACE_TOKEN\n",
    "os.environ['ANTHROPIC_API_KEY'] = ANTHROPIC_API_KEY\n",
    "\n",
    "print(\"‚úì API keys configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Login to Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=HUGGINGFACE_TOKEN)\n",
    "print(\"‚úì Logged in to Hugging Face\")\n",
    "print(\"\\n‚ö†Ô∏è Make sure you've accepted the Llama 3.2 license:\")\n",
    "print(\"   https://huggingface.co/meta-llama/Llama-3.2-1B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "print(\"Testing model loading with 4-bit quantization...\")\n",
    "\n",
    "# 4-bit config (GPU optimized)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load small test\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-3.2-1B\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "\n",
    "print(f\"‚úì Model loaded successfully\")\n",
    "print(f\"  Memory: {model.get_memory_footprint() / 1e9:.2f} GB\")\n",
    "print(f\"  Device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Setup Complete!\n",
    "\n",
    "### What's Ready:\n",
    "- ‚úì GPU enabled and verified\n",
    "- ‚úì Google Drive mounted\n",
    "- ‚úì All packages installed\n",
    "- ‚úì API keys configured\n",
    "- ‚úì Hugging Face authenticated\n",
    "- ‚úì Model loading tested\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "**Continue with these Colab notebooks:**\n",
    "1. `01_colab_data_generation.ipynb` - Generate training data (2-3 hrs)\n",
    "2. `02_colab_training.ipynb` - Train model (1-2 hrs on T4 GPU)\n",
    "3. `03_colab_inference.ipynb` - Test and use the model\n",
    "\n",
    "### Important Notes:\n",
    "- **Save frequently**: Colab disconnects after 12 hours or inactivity\n",
    "- **Models saved to**: Google Drive ‚Üí `LLM_Finetuning/models/`\n",
    "- **Data saved to**: Google Drive ‚Üí `LLM_Finetuning/data/`\n",
    "- **Free GPU limit**: ~15 hours/day on T4 GPU\n",
    "\n",
    "### GPU vs CPU Training Time:\n",
    "- **CPU (local)**: 12-24 hours\n",
    "- **T4 GPU (Colab)**: 1-2 hours ‚ö°\n",
    "\n",
    "**You'll save ~10-20 hours using Colab!**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}