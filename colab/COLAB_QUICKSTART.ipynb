{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/farhan1301/Fine-Tuning-LLM/blob/main/colab/COLAB_QUICKSTART.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0GJragvTHla"
      },
      "source": [
        "# üöÄ Complete LLM Fine-tuning Pipeline - Google Colab\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/farhan1301/Fine-Tuning-LLM/blob/main/colab/COLAB_QUICKSTART.ipynb)\n",
        "\n",
        "**All-in-one notebook** for fine-tuning Llama 3.2 1B on BRD extraction.\n",
        "\n",
        "## ‚ö° Requirements\n",
        "- Google Colab with **T4 GPU** (free tier)\n",
        "- Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí **T4 GPU**\n",
        "\n",
        "## ‚è±Ô∏è Timeline\n",
        "- Setup: 5-10 min\n",
        "- Data Generation: 1-2 hours (or use pre-generated)\n",
        "- Training: 1-2 hours on T4 GPU\n",
        "- **Total: 2-4 hours** (vs 12-24 hours on CPU!)\n",
        "\n",
        "## üìã Before Starting\n",
        "1. Enable GPU runtime\n",
        "2. Get API keys:\n",
        "   - Hugging Face: https://huggingface.co/settings/tokens\n",
        "   - Anthropic: https://console.anthropic.com/\n",
        "3. Accept Llama 3.2 license: https://huggingface.co/meta-llama/Llama-3.2-1B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CymKBy7wTHld"
      },
      "source": [
        "---\n",
        "# Part 1: Setup (5 minutes)\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXKNay5LTHld"
      },
      "source": [
        "## Check GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pk5yJLfoTHle",
        "outputId": "e54e3acc-d2e8-4327-9a18-d79204c8d713",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì GPU Available: Tesla T4\n",
            "‚úì GPU Memory: 15.828320256 GB\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "    raise RuntimeError(\"‚ö†Ô∏è No GPU! Go to: Runtime ‚Üí Change runtime type ‚Üí T4 GPU\")\n",
        "\n",
        "print(\"‚úì GPU Available:\", torch.cuda.get_device_name(0))\n",
        "print(\"‚úì GPU Memory:\", torch.cuda.get_device_properties(0).total_memory / 1e9, \"GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSD02IJRTHlf"
      },
      "source": [
        "## Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dk5hA-tQTHlf"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Project directory in Google Drive\n",
        "PROJECT_DIR = '/content/drive/MyDrive/LLM_Finetuning'\n",
        "os.makedirs(f\"{PROJECT_DIR}/data\", exist_ok=True)\n",
        "os.makedirs(f\"{PROJECT_DIR}/models\", exist_ok=True)\n",
        "os.chdir(PROJECT_DIR)\n",
        "\n",
        "print(f\"‚úì Working directory: {PROJECT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5sl8C3pTHlf"
      },
      "source": [
        "## Install Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51RyR9UiTHlf"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -q -U transformers peft trl bitsandbytes accelerate\n",
        "!pip install -q -U datasets pydantic anthropic gradio\n",
        "!pip install -q 'numpy<2.0'\n",
        "\n",
        "import transformers, peft, trl\n",
        "print(f\"‚úì Transformers: {transformers.__version__}\")\n",
        "print(f\"‚úì PEFT: {peft.__version__}\")\n",
        "print(f\"‚úì TRL: {trl.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aD4uK3dwTHlg"
      },
      "source": [
        "## Configure API Keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORXb_mpCTHlg"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "from getpass import getpass\n",
        "import os\n",
        "\n",
        "# Try Colab secrets first, fallback to manual input\n",
        "try:\n",
        "    HF_TOKEN = userdata.get('HUGGINGFACE_TOKEN')\n",
        "    ANTHROPIC_KEY = userdata.get('ANTHROPIC_API_KEY')\n",
        "    print(\"‚úì Loaded from Colab secrets\")\n",
        "except:\n",
        "    print(\"Enter your API keys:\")\n",
        "    HF_TOKEN = getpass(\"Hugging Face token: \")\n",
        "    ANTHROPIC_KEY = getpass(\"Anthropic API key: \")\n",
        "\n",
        "os.environ['HF_TOKEN'] = HF_TOKEN\n",
        "os.environ['ANTHROPIC_API_KEY'] = ANTHROPIC_KEY\n",
        "\n",
        "# Login to HF\n",
        "from huggingface_hub import login\n",
        "login(token=HF_TOKEN)\n",
        "print(\"‚úì Logged in to Hugging Face\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pK3dsM_wTHlg"
      },
      "source": [
        "---\n",
        "# Part 2: Data Generation (1-2 hours)\n",
        "---\n",
        "\n",
        "**Option A:** Generate 1000 BRDs (1-2 hours, costs ~$3-5 API credits)\n",
        "\n",
        "**Option B:** Use smaller dataset for testing (100 samples, 10-15 min, ~$0.50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDO5W_ArTHlg"
      },
      "outputs": [],
      "source": [
        "# Choose dataset size\n",
        "NUM_SAMPLES = 100  # Change to 1000 for full dataset\n",
        "\n",
        "print(f\"Will generate {NUM_SAMPLES} BRD samples\")\n",
        "print(f\"Estimated time: {NUM_SAMPLES * 0.002:.0f} minutes\")\n",
        "print(f\"Estimated cost: ${NUM_SAMPLES * 0.005:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omfKDKHrTHlh"
      },
      "source": [
        "## Generate Synthetic BRDs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SniJZw4XTHlh"
      },
      "outputs": [],
      "source": [
        "import anthropic\n",
        "import json\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "client = anthropic.Anthropic(api_key=os.environ['ANTHROPIC_API_KEY'])\n",
        "\n",
        "PROJECT_TYPES = [\n",
        "    \"Web Application\", \"Mobile Application\", \"REST API\", \"Data Pipeline\",\n",
        "    \"ML Model\", \"E-commerce Platform\", \"CRM System\", \"Dashboard\"\n",
        "]\n",
        "\n",
        "INDUSTRIES = [\n",
        "    \"Financial Services\", \"Healthcare\", \"E-commerce\", \"Education\",\n",
        "    \"Real Estate\", \"Retail\", \"SaaS\", \"Manufacturing\"\n",
        "]\n",
        "\n",
        "COMPLEXITY = [\n",
        "    {\"level\": \"Simple\", \"hours_range\": (80, 300), \"rate\": 75},\n",
        "    {\"level\": \"Medium\", \"hours_range\": (300, 800), \"rate\": 100},\n",
        "    {\"level\": \"Complex\", \"hours_range\": (800, 2000), \"rate\": 125},\n",
        "]\n",
        "\n",
        "def generate_brd(project_type, industry, complexity, team_size):\n",
        "    effort_hours = random.randint(*complexity[\"hours_range\"])\n",
        "    hourly_rate = complexity[\"rate\"] + random.randint(-15, 15)\n",
        "    cost_usd = effort_hours * hourly_rate\n",
        "    hours_per_week = team_size * 40\n",
        "    timeline_weeks = max(1, round(effort_hours / hours_per_week))\n",
        "\n",
        "    prompt = f\"\"\"Generate a realistic 2-3 paragraph Business Requirements Document for a {complexity['level'].lower()} {project_type.lower()} in {industry}.\n",
        "\n",
        "Include:\n",
        "- Project overview and objectives\n",
        "- Technical scope and features\n",
        "- Resource needs ({team_size} team members)\n",
        "- Timeline: approximately {timeline_weeks} weeks\n",
        "- Effort: approximately {effort_hours} hours total\n",
        "- Budget: approximately ${cost_usd:,}\n",
        "\n",
        "Write in professional prose, not template format. Use natural variations in terminology.\"\"\"\n",
        "\n",
        "    try:\n",
        "        message = client.messages.create(\n",
        "            model=\"claude-3-5-sonnet-20241022\",\n",
        "            max_tokens=1000,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "        )\n",
        "        return {\n",
        "            \"brd_text\": message.content[0].text,\n",
        "            \"labels\": {\n",
        "                \"effort_hours\": float(effort_hours),\n",
        "                \"timeline_weeks\": int(timeline_weeks),\n",
        "                \"cost_usd\": float(cost_usd)\n",
        "            }\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return None\n",
        "\n",
        "# Generate dataset\n",
        "dataset = []\n",
        "for i in tqdm(range(NUM_SAMPLES)):\n",
        "    brd = generate_brd(\n",
        "        random.choice(PROJECT_TYPES),\n",
        "        random.choice(INDUSTRIES),\n",
        "        random.choice(COMPLEXITY),\n",
        "        random.choice([1,2,3,4,5])\n",
        "    )\n",
        "    if brd:\n",
        "        brd[\"id\"] = i\n",
        "        dataset.append(brd)\n",
        "    time.sleep(0.5)\n",
        "\n",
        "print(f\"\\n‚úì Generated {len(dataset)} BRDs\")\n",
        "\n",
        "# Save\n",
        "with open(f\"{PROJECT_DIR}/data/dataset.json\", \"w\") as f:\n",
        "    json.dump(dataset, f, indent=2)\n",
        "print(f\"‚úì Saved to {PROJECT_DIR}/data/dataset.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxfxdYibTHlh"
      },
      "source": [
        "## Format Data for Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63SNDyJVTHlh"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Format for instruction tuning\n",
        "formatted_data = []\n",
        "for sample in dataset:\n",
        "    text = f\"\"\"### Instruction:\n",
        "Extract the project estimation fields from the following Business Requirements Document.\n",
        "Return a JSON object with these exact fields: effort_hours (number), timeline_weeks (number), cost_usd (number).\n",
        "Return ONLY the JSON object, no additional text.\n",
        "\n",
        "### Input:\n",
        "{sample['brd_text']}\n",
        "\n",
        "### Output:\n",
        "{json.dumps(sample['labels'])}\"\"\"\n",
        "    formatted_data.append({\"text\": text, \"id\": sample[\"id\"]})\n",
        "\n",
        "# Split\n",
        "train, temp = train_test_split(formatted_data, test_size=0.2, random_state=42)\n",
        "val, test = train_test_split(temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Save\n",
        "for name, data in [(\"train\", train), (\"val\", val), (\"test\", test)]:\n",
        "    with open(f\"{PROJECT_DIR}/data/{name}.json\", \"w\") as f:\n",
        "        json.dump(data, f)\n",
        "\n",
        "print(f\"‚úì Train: {len(train)} | Val: {len(val)} | Test: {len(test)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCH8LUqkTHlh"
      },
      "source": [
        "---\n",
        "# Part 3: Model Training (1-2 hours on T4 GPU)\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zc9dWoKkTHlh"
      },
      "source": [
        "## Load Model with 4-bit Quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMgJlcKeTHli"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from trl import SFTTrainer\n",
        "from datasets import load_dataset\n",
        "\n",
        "# 4-bit quantization (GPU optimized)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "print(\"Loading Llama 3.2 1B...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"meta-llama/Llama-3.2-1B\",\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "print(f\"‚úì Model loaded: {model.get_memory_footprint() / 1e9:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYZEKKuVTHli"
      },
      "source": [
        "## Configure LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-eSAlx9THli"
      },
      "outputs": [],
      "source": [
        "# Prepare model\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# LoRA config (GPU optimized - higher rank than CPU)\n",
        "lora_config = LoraConfig(\n",
        "    r=16,  # Higher rank for GPU (was 8 for CPU)\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "print(\"‚úì LoRA configured\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwiuidQUTHli"
      },
      "source": [
        "## Load Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jA06PA19THli"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"json\", data_files={\n",
        "    \"train\": f\"{PROJECT_DIR}/data/train.json\",\n",
        "    \"validation\": f\"{PROJECT_DIR}/data/val.json\",\n",
        "})\n",
        "\n",
        "print(f\"‚úì Train: {len(dataset['train'])} | Val: {len(dataset['validation'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djXTa-RpTHli"
      },
      "source": [
        "## Configure Training (GPU Optimized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJPTcNBdTHli"
      },
      "outputs": [],
      "source": [
        "OUTPUT_DIR = f\"{PROJECT_DIR}/models/llama-3.2-1b-brd\"\n",
        "\n",
        "# GPU-optimized training args\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,  # Larger than CPU (was 1)\n",
        "    gradient_accumulation_steps=4,   # Smaller (was 32)\n",
        "    learning_rate=2e-4,\n",
        "    warmup_steps=100,\n",
        "    logging_steps=10,\n",
        "    save_steps=50,\n",
        "    eval_steps=50,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    fp16=True,  # GPU supports fp16!\n",
        "    gradient_checkpointing=True,\n",
        "    optim=\"adamw_torch\",\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    max_seq_length=2048,\n",
        "    dataset_text_field=\"text\",\n",
        "    packing=False,\n",
        ")\n",
        "\n",
        "print(\"‚úì Trainer initialized\")\n",
        "print(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
        "print(f\"  Total steps: {trainer.args.max_steps}\")\n",
        "print(f\"  Estimated time: 1-2 hours on T4 GPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sj2MONgTHli"
      },
      "source": [
        "## Start Training!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TyWwdJBATHlj"
      },
      "outputs": [],
      "source": [
        "print(\"üöÄ Starting training...\\n\")\n",
        "\n",
        "# Train\n",
        "trainer.train()\n",
        "\n",
        "# Save\n",
        "trainer.save_model(f\"{OUTPUT_DIR}/final\")\n",
        "tokenizer.save_pretrained(f\"{OUTPUT_DIR}/final\")\n",
        "\n",
        "print(\"\\n‚úì Training complete!\")\n",
        "print(f\"  Model saved to: {OUTPUT_DIR}/final\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idUdOAkOTHlj"
      },
      "source": [
        "---\n",
        "# Part 4: Test the Model\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oU1bJb_4THlj"
      },
      "outputs": [],
      "source": [
        "test_brd = \"\"\"Business Requirements Document\n",
        "Project: E-commerce Mobile App\n",
        "\n",
        "We need a cross-platform mobile app for our e-commerce business with product browsing,\n",
        "shopping cart, and secure checkout. The project requires 3 developers for 12 weeks.\n",
        "Total estimated effort is 720 hours with a budget of $90,000.\"\"\"\n",
        "\n",
        "prompt = f\"\"\"### Instruction:\n",
        "Extract the project estimation fields from the following Business Requirements Document.\n",
        "Return a JSON object with these exact fields: effort_hours (number), timeline_weeks (number), cost_usd (number).\n",
        "Return ONLY the JSON object, no additional text.\n",
        "\n",
        "### Input:\n",
        "{test_brd}\n",
        "\n",
        "### Output:\n",
        "\"\"\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model.generate(**inputs, max_new_tokens=100, temperature=0.1, do_sample=True)\n",
        "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Extracted Output:\")\n",
        "print(\"=\"*60)\n",
        "print(result.split(\"### Output:\")[-1].strip())\n",
        "print(\"=\"*60)\n",
        "print(\"\\n‚úì Model working!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unnmCqI-THlj"
      },
      "source": [
        "---\n",
        "# üéâ Complete!\n",
        "---\n",
        "\n",
        "### What You've Built:\n",
        "- ‚úì Generated synthetic training data\n",
        "- ‚úì Fine-tuned Llama 3.2 1B with QLoRA\n",
        "- ‚úì Trained on T4 GPU in ~1-2 hours\n",
        "- ‚úì Model extracts structured JSON from BRDs\n",
        "\n",
        "### Your Model:\n",
        "- Location: `Google Drive/LLM_Finetuning/models/llama-3.2-1b-brd/final`\n",
        "- Size: ~10-50 MB (LoRA adapters only)\n",
        "- Can be loaded and used anytime\n",
        "\n",
        "### Next Steps:\n",
        "1. **Download model** from Google Drive\n",
        "2. **Share on Hugging Face** for portfolio\n",
        "3. **Use in production** with Pydantic validation\n",
        "4. **Create demo** with Gradio\n",
        "\n",
        "### Load Model Later:\n",
        "```python\n",
        "from transformers import AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "\n",
        "base = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
        "model = PeftModel.from_pretrained(base, \"path/to/final\")\n",
        "```\n",
        "\n",
        "**Congratulations! üöÄ You've fine-tuned an LLM!**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}